{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IshmamF/2023-fall-data-science-fridays/blob/main/PrimeHackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Check GPU type** ğŸ•µï¸\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QshUbLqpX7L4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20f079f-57b3-4bd4-a87b-5cae0d10d141"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-a20abac0-b220-7698-193a-dc67448b6423)\n",
            "Sun Nov 19 14:22:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc129c7-c469-4935-ab7e-5f8572ee11db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-nw27hx44\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-nw27hx44\n",
            "  Resolved https://github.com/openai/whisper.git to commit e58f28804528831904c3b6f2c0e473f346223433\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=dc0d45af989b661e5c1580aed2852e3b43d77abb7bb6db498e1da0a9dfbc1a6e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g3v9igoc/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.1\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.11.16-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.4)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-12.0 yt-dlp-2023.11.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Install libraries** ğŸ—ï¸\n",
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07445c4f-bd44-4a09-aa10-d0abe649f11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive ğŸ’¾\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** ğŸ§ \n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'large' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))"
      ],
      "metadata": {
        "id": "TMhrSq_GZ6kA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "6d2625c9-5331-45f9-d55f-9ca67b4ff539"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [00:37<00:00, 81.9MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**large model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok==4.1.1"
      ],
      "metadata": {
        "id": "FD8eKL9lVmVh",
        "outputId": "c6a24a0f-cab8-41d8-edd8-3178c166438a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.28.2 validators-0.22.0 watchdog-3.0.0\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15964 sha256=2a7994823e7328591670dd4990bcc31efbdc365dfdc14facf59f4efe6cab8d3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7c/4c/632fba2ea8e88d8890102eb07bc922e1ca8fa14db5902c91a8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api"
      ],
      "metadata": {
        "id": "NLKd_Kcoe4ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLFhaPNggnNX",
        "outputId": "64301189-643a-425f-c952-f2d828364412"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "import time\n",
        "import tempfile\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "from transformers import pipeline\n",
        "\n",
        "def Summarizer_function(transcription):\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    def word_length(paragraph):\n",
        "      words = paragraph.split()\n",
        "      return len(words)\n",
        "    min_length = word_length(transcription)\n",
        "    if min_length >= 1800:\n",
        "    # Assuming 'summarizer' is a function you have defined elsewhere\n",
        "      summary = summarizer(transcription, min_length=150, do_sample=False)\n",
        "    elif min_length < 1800:\n",
        "    # Assuming 'summarizer' is a function you have defined elsewhere\n",
        "      summary = summarizer(transcription, min_length=50, do_sample=False)\n",
        "      return summary\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "Model = 'tiny' # ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "st.title(\"Language Alchemy\")\n",
        "\n",
        "st.header('Translate any youtube video in the language of your choice!')\n",
        "\n",
        "def main():\n",
        "  youtube_link = st.text_input(\"Enter YouTube Video Link\")\n",
        "  choice = st.selectbox('Select', ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba'])\n",
        "  button = st.button('Submit')\n",
        "\n",
        "  if button and youtube_link and choice:\n",
        "\n",
        "    Type = \"Youtube video or playlist\"\n",
        "    URL = youtube_link\n",
        "    video_path = \"Colab Notebooks/transcription/my_video.mp4\"\n",
        "\n",
        "    video_path_local_list = []\n",
        "\n",
        "    if Type == \"Youtube video or playlist\":\n",
        "\n",
        "      ydl_opts = {\n",
        "          'format': 'm4a/bestaudio/best',\n",
        "          'outtmpl': '%(id)s.%(ext)s',\n",
        "          # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "          'postprocessors': [{  # Extract audio using ffmpeg\n",
        "              'key': 'FFmpegExtractAudio',\n",
        "              'preferredcodec': 'wav',\n",
        "          }]\n",
        "      }\n",
        "\n",
        "      with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "          error_code = ydl.download([URL])\n",
        "          list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "      for video_info in list_video_info:\n",
        "          video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "    elif Type == \"Google Drive\":\n",
        "      # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "      video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "      if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "          if video_path_drive.is_file():\n",
        "            display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "          elif video_path_drive.is_dir():\n",
        "            display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "          else:\n",
        "            display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "          video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "          shutil.copy(video_path_drive, video_path_local)\n",
        "          video_path_local_list.append(video_path_local)\n",
        "      elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "      else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "    else:\n",
        "      raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "    for video_path_local in video_path_local_list:\n",
        "      if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n",
        "\n",
        "\n",
        "    language = choice\n",
        "    verbose = 'Live transcription'\n",
        "    output_format = 'all'\n",
        "    task = 'translate'\n",
        "    temperature = 0.15\n",
        "    temperature_increment_on_fallback = 0.2\n",
        "    best_of = 5\n",
        "    beam_size = 8\n",
        "    patience = 1.0\n",
        "    length_penalty = -0.05\n",
        "    suppress_tokens = \"-1\"\n",
        "    initial_prompt = \"\"\n",
        "    condition_on_previous_text = True\n",
        "    fp16 = True\n",
        "    compression_ratio_threshold = 2.4\n",
        "    logprob_threshold = -1.0\n",
        "    no_speech_threshold = 0.6\n",
        "\n",
        "    verbose_lut = {\n",
        "        'Live transcription': True,\n",
        "        'Progress bar': False,\n",
        "        'None': None\n",
        "    }\n",
        "\n",
        "    args = dict(\n",
        "        language = (None if language == \"Auto detection\" else language),\n",
        "        verbose = verbose_lut[verbose],\n",
        "        task = task,\n",
        "        temperature = temperature,\n",
        "        temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "        best_of = best_of,\n",
        "        beam_size = beam_size,\n",
        "        patience=patience,\n",
        "        length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "        suppress_tokens=suppress_tokens,\n",
        "        initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "        condition_on_previous_text=condition_on_previous_text,\n",
        "        fp16=fp16,\n",
        "        compression_ratio_threshold=compression_ratio_threshold,\n",
        "        logprob_threshold=logprob_threshold,\n",
        "        no_speech_threshold=no_speech_threshold\n",
        "    )\n",
        "\n",
        "    temperature = args.pop(\"temperature\")\n",
        "    temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "    if temperature_increment_on_fallback is not None:\n",
        "        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "    else:\n",
        "        temperature = [temperature]\n",
        "\n",
        "    if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "        warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "        args[\"language\"] = \"en\"\n",
        "\n",
        "    for video_path_local in video_path_local_list:\n",
        "        display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "        video_transcription = whisper.transcribe(\n",
        "            whisper_model,\n",
        "            str(video_path_local),\n",
        "            temperature=temperature,\n",
        "            **args,\n",
        "        )\n",
        "\n",
        "        # Save output\n",
        "        whisper.utils.get_writer(\n",
        "            output_format=output_format,\n",
        "            output_dir=video_path_local.parent\n",
        "        )(\n",
        "            video_transcription,\n",
        "            str(video_path_local.stem),\n",
        "            options=dict(\n",
        "                highlight_words=False,\n",
        "                max_line_count=None,\n",
        "                max_line_width=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        def exportTranscriptFile(ext: str):\n",
        "            local_path = video_path_local.parent / video_path_local.with_suffix(ext)\n",
        "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                temp_path = temp_file.name\n",
        "                shutil.copy(local_path, temp_path)\n",
        "                shutil.move(temp_path, local_path)\n",
        "\n",
        "        exportTranscriptFile('.txt')\n",
        "\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    col1, col2 = st.columns([1,1])\n",
        "    with col1:\n",
        "      st.video(youtube_link)\n",
        "    with col2:\n",
        "      with st.spinner(text='In progress'):\n",
        "        max_wait_time = 75  # You can adjust this value based on your needs\n",
        "        start_time = time.time()\n",
        "\n",
        "        while not Path(f\"{video_info['id']}.txt\").is_file():\n",
        "            if time.time() - start_time > max_wait_time:\n",
        "                st.warning(\"Timeout: Unable to load the file within the specified time.\")\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Assuming video_info['id'] is a placeholder for your actual video ID\n",
        "        video_file_path = f\"{video_info['id']}.txt\"\n",
        "\n",
        "        if Path(video_file_path).is_file():\n",
        "            video_file = open(video_file_path, 'r')\n",
        "            text_content = video_file.read()\n",
        "            st.text_area(\"Transcription:\", text_content, height=225)  # Adjust height as needed\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"File {video_file_path} not found.\")\n",
        "    st.subheader('Summary:')\n",
        "    with st.spinner(text=\"In progress...\"):\n",
        "      summary = Summarizer_function(text_content)[0][\"summary_text\"]\n",
        "      st.write(summary)\n",
        "      time.sleep(2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CFG6YjApOAnq",
        "outputId": "1fcf19bc-85f6-4ee1-a52c-633690ec0c08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "djY30i26iQtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('2YNwFxrosM2EhDHzpfZt8RP4ozL_57TYy1E5dAo47h7XKmyXX')\n"
      ],
      "metadata": {
        "id": "DILKbsZeWrrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe62e8aa-3dcb-49ed-f643-cca76d6f6e57"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Set up a new tunnel\n",
        "site = ngrok.connect(port=8501)\n",
        "\n",
        "# Run the Streamlit app as a background process\n",
        "!nohup streamlit run app.py &\n",
        "\n",
        "# Print the public URL to access the Streamlit app\n",
        "print('Streamlit URL:', site)"
      ],
      "metadata": {
        "id": "_ohMtLBma4c3",
        "outputId": "dfe17ebe-c513-41ad-ceb1-3749dde1fd10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Streamlit URL: http://9cee-34-126-181-240.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KN9cTDwjIRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8888\n"
      ],
      "metadata": {
        "id": "1siOr4kwjCeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gr-_s17SjE9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** ğŸ“º\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://www.youtube.com/watch?v=5Oqz3y2Kbzs\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        error_code = ydl.download([URL])\n",
        "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "    for video_info in list_video_info:\n",
        "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "elif Type == \"Google Drive\":\n",
        "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "            if video_path_drive.is_file():\n",
        "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "            elif video_path_drive.is_dir():\n",
        "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "            else:\n",
        "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "            shutil.copy(video_path_drive, video_path_local)\n",
        "            video_path_local_list.append(video_path_local)\n",
        "    elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "    else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "else:\n",
        "    raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b54398-f9fc-49c0-b678-a469228d0fdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=5Oqz3y2Kbzs\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading webpage\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading ios player API JSON\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading android player API JSON\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading player 190c935f\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading m3u8 information\n",
            "[info] 5Oqz3y2Kbzs: Downloading 1 format(s): 140\n",
            "[download] Destination: 5Oqz3y2Kbzs.m4a\n",
            "[download] 100% of    4.81MiB in 00:00:00 at 19.97MiB/s  \n",
            "[FixupM4a] Correcting container of \"5Oqz3y2Kbzs.m4a\"\n",
            "[ExtractAudio] Destination: 5Oqz3y2Kbzs.wav\n",
            "Deleting original file 5Oqz3y2Kbzs.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=5Oqz3y2Kbzs\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading webpage\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading ios player API JSON\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading android player API JSON\n",
            "[youtube] 5Oqz3y2Kbzs: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-X0qB9JAzMLY",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cdb473ec-14e0-4d34-8707-15676842ea6a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### 5Oqz3y2Kbzs.wav"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:03.520]  So now let's implement the adapter design pattern.\n",
            "[00:03.520 --> 00:09.600]  It's a structural pattern that basically allows a couple incompatible classes to work together.\n",
            "[00:09.600 --> 00:13.880]  And it does so usually by wrapping a class behind a new class.\n",
            "[00:13.880 --> 00:19.120]  Now, in this case, we're going to be dealing with a relatively simple example.\n",
            "[00:19.120 --> 00:20.760]  So we're given a few classes.\n",
            "[00:20.760 --> 00:25.600]  We're given a square hole class, a square class and a circle class.\n",
            "[00:25.600 --> 00:29.520]  Of course, a square can fit into a square hole,\n",
            "[00:29.520 --> 00:35.360]  but only if the length of the square, the side length is less than or equal to the side\n",
            "[00:35.360 --> 00:37.000]  length of the square hole.\n",
            "[00:37.000 --> 00:41.920]  I'll scroll down so you can see the picture because pictures are usually easier to understand.\n",
            "[00:41.920 --> 00:46.300]  Now a circle doesn't have a side length, but it does have a radius.\n",
            "[00:46.300 --> 00:53.240]  And for a circle to fit into a square like this one, the circles diameter has to be less\n",
            "[00:53.240 --> 00:56.360]  than or equal to the square holes side length.\n",
            "[00:56.360 --> 00:59.020]  And the diameter of a circle is of course,\n",
            "[00:59.020 --> 00:59.520]  two times,\n",
            "[00:59.520 --> 01:05.180]  the radius and you can kind of imply as much from this picture,\n",
            "[01:05.180 --> 01:11.240]  as long as two times the radius of the circle is less than the side length of the square,\n",
            "[01:11.240 --> 01:12.960]  it should fit inside.\n",
            "[01:12.960 --> 01:16.660]  That's going to be important because what we are implementing here is going to be a\n",
            "[01:16.660 --> 01:19.620]  circle to square adapter,\n",
            "[01:19.620 --> 01:25.500]  such that it allows us to put circles inside of square holes quickly down here,\n",
            "[01:25.500 --> 01:27.700]  looking at some of the example code,\n",
            "[01:27.700 --> 01:29.520]  you can see we have a square hole.\n",
            "[01:29.520 --> 01:33.600]  The parameter it accepts is going to be the side length of that square hole.\n",
            "[01:33.600 --> 01:40.380]  And it's going to have a method to determine if a square can fit into that square hole.\n",
            "[01:40.380 --> 01:41.380]  And in this case,\n",
            "[01:41.380 --> 01:45.240]  it can we return true and that's because look at the square that we constructed and then\n",
            "[01:45.240 --> 01:46.240]  passed in,\n",
            "[01:46.240 --> 01:48.600]  it also has a side length of five.\n",
            "[01:48.600 --> 01:51.260]  And within the square hole code,\n",
            "[01:51.260 --> 01:56.140]  you can see here that the way we would determine if the square can fit or not is basically\n",
            "[01:56.140 --> 01:59.360]  the side length of the square hole is greater than or equal to the side.\n",
            "[01:59.360 --> 02:00.820]  Length of the square.\n",
            "[02:00.820 --> 02:04.540]  So that's pretty easy from the squares perspective.\n",
            "[02:04.540 --> 02:06.680]  And just looking at the square code quickly,\n",
            "[02:06.680 --> 02:07.680]  it's pretty simple,\n",
            "[02:07.680 --> 02:12.640]  has a constructor that allows us to set the side length and then get the side length as\n",
            "[02:12.640 --> 02:13.640]  well,\n",
            "[02:13.640 --> 02:14.640]  which of course,\n",
            "[02:14.640 --> 02:18.120]  that get side length is what is used in the square hole class.\n",
            "[02:18.120 --> 02:19.120]  And down here,\n",
            "[02:19.120 --> 02:20.120]  though,\n",
            "[02:20.120 --> 02:21.120]  we also have a circle,\n",
            "[02:21.120 --> 02:22.120]  of course,\n",
            "[02:22.120 --> 02:24.120]  that doesn't have a side length that has a radius.\n",
            "[02:24.120 --> 02:29.120]  So that's why we can't pass a circle into the can fit method.\n",
            "[02:29.120 --> 02:30.120]  First of all,\n",
            "[02:30.120 --> 02:31.880]  it expects a square interface.\n",
            "[02:31.880 --> 02:37.420]  And that's because squares have get side length circles do not have that.\n",
            "[02:37.420 --> 02:43.800]  So how do we make this circle compatible with the square hole class,\n",
            "[02:43.800 --> 02:46.120]  and not only make it compatible,\n",
            "[02:46.120 --> 02:48.460]  but also make it accurate?\n",
            "[02:48.460 --> 02:53.760]  How do we make it so that when we call get side length on whatever adapter we use for\n",
            "[02:53.760 --> 02:54.760]  the circle,\n",
            "[02:54.760 --> 02:58.400]  that it's actually accurate actually tells us if the circle fits or not,\n",
            "[02:58.400 --> 03:00.340]  because looking at this example here,\n",
            "[03:00.340 --> 03:04.080]  you can see if we create a circle with a radius of five,\n",
            "[03:04.080 --> 03:07.160]  and then with our all I'll make it a bit larger,\n",
            "[03:07.160 --> 03:09.620]  but with our circle to square adapter,\n",
            "[03:09.620 --> 03:11.240]  we pass that circle in,\n",
            "[03:11.240 --> 03:15.500]  and this circle to square adapter will be passed into the square hole,\n",
            "[03:15.500 --> 03:16.760]  and it returns false,\n",
            "[03:16.760 --> 03:23.100]  it does not fit because a circle with a radius of five is going to have a diameter of 10.\n",
            "[03:23.100 --> 03:25.400]  There's no way that's going to fit in a square hole,\n",
            "[03:25.400 --> 03:27.160]  but the side length of five down here,\n",
            "[03:27.160 --> 03:28.360]  the example is kind of the opposite.\n",
            "[03:28.360 --> 03:30.280]  The square is too big.\n",
            "[03:30.280 --> 03:31.500]  It does not fit,\n",
            "[03:31.500 --> 03:34.520]  but the circle is small enough such that it does fit.\n",
            "[03:34.520 --> 03:39.760]  So we want to create an adapter that allows us to pass circles into the square hole.\n",
            "[03:39.760 --> 03:41.220]  Let's go ahead and do that.\n",
            "[03:41.220 --> 03:45.160]  Now let's go ahead and implement this circle to square adapter.\n",
            "[03:45.160 --> 03:46.160]  Of course,\n",
            "[03:46.160 --> 03:47.960]  the constructor is going to take a circle in,\n",
            "[03:47.960 --> 03:48.960]  and for that,\n",
            "[03:48.960 --> 03:53.860]  we're going to need to have a variable to store that circle.\n",
            "[03:53.860 --> 03:57.900]  So that's basically what the responsibility of the constructor is going to be here.\n",
            "[03:57.900 --> 04:00.840]  It's going to store whatever circle we passed in.\n",
            "[04:00.840 --> 04:05.000]  So this is kind of abstracting that this is wrapping that existing circle.\n",
            "[04:05.000 --> 04:09.660]  And notice that this circle to square adapter does extend the square,\n",
            "[04:09.660 --> 04:14.900]  that is why we are overriding the get side length method.\n",
            "[04:14.900 --> 04:18.980]  Because then when we pass this into this method here,\n",
            "[04:18.980 --> 04:20.220]  that can fit method,\n",
            "[04:20.220 --> 04:21.900]  it will be casted to a square,\n",
            "[04:21.900 --> 04:23.760]  we can call get side length on it,\n",
            "[04:23.760 --> 04:27.280]  and we will accurately be able to determine if the circle fits or not.\n",
            "[04:27.280 --> 04:27.900]  But now,\n",
            "[04:27.900 --> 04:34.980]  us to actually know what the side length is, well, the proxy or the equivalent way of saying\n",
            "[04:34.980 --> 04:39.080]  like the side length of a circle is basically two times the radius. So that's what we're\n",
            "[04:39.080 --> 04:44.860]  going to do here, we're gonna say this circle, get the radius and multiply by two. And that\n",
            "[04:44.860 --> 04:48.680]  is going to be the side length, that is what we can use to determine if a circle will fit\n",
            "[04:48.680 --> 04:53.900]  in the square hole or not. So that is the entire code. Let's go ahead and put a semi\n",
            "[04:53.900 --> 04:58.320]  colon there and run this to make sure that it works. And as you can see, yes, it does.\n",
            "[04:58.320 --> 05:02.000]  So I'll go ahead and leave things there. But I think this was definitely one of the more\n",
            "[05:02.000 --> 05:06.440]  interesting problems. And it might have been a bit challenging as well, because some of\n",
            "[05:06.440 --> 05:11.380]  this is not exclusive to like the adapter. But I think it does make sense in this context.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/5Oqz3y2Kbzs.txt**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/5Oqz3y2Kbzs.vtt**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/5Oqz3y2Kbzs.srt**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/5Oqz3y2Kbzs.tsv**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/5Oqz3y2Kbzs.json**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # **Run the model** ğŸš€\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** âš™ï¸\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"Bengali\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'all' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'translate' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".es\") and args[\"language\"] not in {\"es\", \"Spanish\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"es\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def exportTranscriptFile(ext: str):\n",
        "        local_path = video_path_local.parent / video_path_local.with_suffix(ext)\n",
        "        export_path = drive_whisper_path / video_path_local.with_suffix(ext)\n",
        "        shutil.copy(\n",
        "            local_path,\n",
        "            export_path\n",
        "        )\n",
        "        display(Markdown(f\"**Transcript file created: {export_path}**\"))\n",
        "\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('.txt', '.vtt', '.srt', '.tsv', '.json'):\n",
        "            exportTranscriptFile(ext)\n",
        "    else:\n",
        "        exportTranscriptFile(\".\" + output_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}